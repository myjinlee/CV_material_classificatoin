{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Ekq5x_HciiTe","executionInfo":{"status":"ok","timestamp":1765687169280,"user_tz":-540,"elapsed":23345,"user":{"displayName":"MyeongJin Lee","userId":"03843740464656886981"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e73e7969-0d2d-4e80-cae6-71e9e0df214e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n","# THEN FEEL FREE TO DELETE THIS CELL.\n","# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n","# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n","# NOTEBOOK.\n","import kagglehub\n","liewyousheng_minc2500_path = kagglehub.dataset_download('liewyousheng/minc2500')\n","# feyzazkefe_trashnet_path = kagglehub.dataset_download('feyzazkefe/trashnet')\n","# youssefhazemfarouk_sdscsc_path = kagglehub.dataset_download('youssefhazemfarouk/sdscsc')\n","# mazennafe3_trashnetttt_path = kagglehub.dataset_download('mazennafe3/trashnetttt')\n","# mazennafe3_trashdata_path = kagglehub.dataset_download('mazennafe3/trashdata')\n","\n","print('Data source import complete.')\n","\n","base_path = liewyousheng_minc2500_path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nX4-BlO4fGrN","executionInfo":{"status":"ok","timestamp":1765687173417,"user_tz":-540,"elapsed":4130,"user":{"displayName":"MyeongJin Lee","userId":"03843740464656886981"}},"outputId":"e8e1fed5-2460-4b1d-90b8-56bbc429ae78"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Using Colab cache for faster access to the 'minc2500' dataset.\n","Data source import complete.\n"]}]},{"cell_type":"code","source":["# Define source directory and the new directory to hold selected classes\n","import shutil\n","import os\n","# source_dir = \"/kaggle/input/minc2500/minc-2500/images\"\n","source_dir = os.path.join(base_path, \"minc-2500\", \"images\")\n","# selected_classes = ['plastic', 'glass', 'metal', 'wood', 'paper']#,'water','polishedstone']\n","\n","selected_classes = ['brick', 'carpet', 'ceramic', 'fabric', 'foliage', 'food', 'glass', 'hair', 'leather',\n","                    'metal', 'mirror', 'other', 'painted', 'paper', 'plastic', 'polishedstone', 'skin',\n","                    'sky', 'stone', 'tile', 'wallpaper', 'water', 'wood']\n","\n","new_dir = \"/kaggle/working/selected_images\"\n","\n","# Create new directory for selected classes\n","os.makedirs(new_dir, exist_ok=True)\n","\n","# Copy only the selected classes to the new directory\n","for class_name in selected_classes:\n","    class_dir = os.path.join(source_dir, class_name)\n","    if os.path.isdir(class_dir):\n","        shutil.copytree(class_dir, os.path.join(new_dir, class_name))\n","\n","# Now use splitfolders to split the selected classes\n"],"metadata":{"id":"-C-kaQX0fSuL","executionInfo":{"status":"ok","timestamp":1765687696370,"user_tz":-540,"elapsed":522949,"user":{"displayName":"MyeongJin Lee","userId":"03843740464656886981"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4238,"status":"ok","timestamp":1765687700615,"user":{"displayName":"MyeongJin Lee","userId":"03843740464656886981"},"user_tz":-540},"id":"2MGDD6NMR1Er","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3d1f1d5a-9ab3-42cf-fdf3-b9f552d95407"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting split-folders\n","  Downloading split_folders-0.5.1-py3-none-any.whl.metadata (6.2 kB)\n","Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n","Installing collected packages: split-folders\n","Successfully installed split-folders-0.5.1\n"]}],"source":["# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n","!pip install split-folders"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":51291,"status":"ok","timestamp":1765687751911,"user":{"displayName":"MyeongJin Lee","userId":"03843740464656886981"},"user_tz":-540},"id":"qlfFqkIaRQTm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"964037fa-56c0-4a01-ff8f-6468ced6fe1a"},"outputs":[{"output_type":"stream","name":"stderr","text":["Copying files: 57500 files [00:32, 1762.27 files/s]"]},{"output_type":"stream","name":"stdout","text":["Selected classes have been split into train, validation, andÂ testÂ sets.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","import shutil\n","import timm\n","import splitfolders\n","import os\n","\n","splitfolders.ratio(new_dir, output=\"/kaggle/working/Splitted\", seed=1337, ratio=(0.85, 0.05, 0.10))\n","\n","print(\"Selected classes have been split into train, validation, andÂ testÂ sets.\")\n","\n","train_path = '/kaggle/working/Splitted/train'\n","val_path = '/kaggle/working/Splitted/val'\n","test_path = '/kaggle/working/Splitted/test'"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import timm\n","from torchvision.models import DenseNet121_Weights, densenet121\n","import os\n","\n","class FrequencyFeatureExtractor(nn.Module):\n","    \"\"\"\n","    2D FFTë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì˜ í¬ê¸°(Magnitude) ìŠ¤íŽ™íŠ¸ëŸ¼ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³  ì²˜ë¦¬í•˜ëŠ” ëª¨ë“ˆ\n","    \"\"\"\n","    def __init__(self, output_dim, image_size=224):\n","        super(FrequencyFeatureExtractor, self).__init__()\n","        self.image_size = image_size\n","\n","        self.spectrum_processor = nn.Sequential(\n","            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),\n","            nn.AdaptiveAvgPool2d((1, 1))\n","        )\n","\n","        # ìµœì¢…ì ìœ¼ë¡œ ViT ìž„ë² ë”© ì°¨ì›(output_dim)ì— ë§žì¶”ê¸° ìœ„í•œ íˆ¬ì˜ ë ˆì´ì–´\n","        self.projection = nn.Linear(64, output_dim)\n","\n","    def forward(self, x):\n","        # 1. RGB ì´ë¯¸ì§€ë¥¼ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜\n","        x_gray = torch.mean(x, dim=1, keepdim=True)\n","\n","        # 2. 2D FFT ì ìš© ë° ì¤‘ì‹¬ ì´ë™\n","        f = torch.fft.fft2(x_gray, dim=(-2, -1))\n","        f_shifted = torch.fft.fftshift(f, dim=(-2, -1))\n","\n","        # 3. í¬ê¸° ìŠ¤íŽ™íŠ¸ëŸ¼ (Magnitude Spectrum) ê³„ì‚°\n","        magnitude_spectrum = torch.log(1 + torch.abs(f_shifted))\n","\n","        # 4. ìŠ¤íŽ™íŠ¸ëŸ¼ ì²˜ë¦¬ ë° ë²¡í„°í™”\n","        processed_features = self.spectrum_processor(magnitude_spectrum)\n","        processed_features = torch.flatten(processed_features, 1)\n","\n","        # 5. ViT ìž„ë² ë”© ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜\n","        frequency_features = self.projection(processed_features)\n","\n","        return frequency_features\n","\n","\n","class FineTunedFusionClassifier(nn.Module):\n","    def __init__(self, num_classes=23, vit_model_name='vit_base_patch16_224', vit_weight_path=None):\n","        super(FineTunedFusionClassifier, self).__init__()\n","\n","        # 1. ViT ëª¨ë¸ (timm)\n","        self.vit_model = timm.create_model(\n","            vit_model_name,\n","            pretrained=False,\n","            num_classes=0,\n","        )\n","        self.vit_embed_dim = self.vit_model.num_features # 768\n","\n","        # ViT ê°€ì¤‘ì¹˜ ë¡œë“œ\n","        if vit_weight_path and os.path.exists(vit_weight_path):\n","            print(f\"INFO: Custom ViT weights loading from {vit_weight_path}...\")\n","            custom_weights = torch.load(vit_weight_path)\n","            self.vit_model.load_state_dict(custom_weights, strict=False)\n","\n","        # 2. DenseNet íŠ¹ì§• ì¶”ì¶œê¸°\n","        self.cnn_model = densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)\n","        self.cnn_features = self.cnn_model.features\n","        self.cnn_feature_dim = self.cnn_model.classifier.in_features # 1024\n","\n","        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n","\n","        # projection\n","        self.cnn_projection = nn.Sequential(\n","            nn.Linear(self.cnn_feature_dim, self.vit_embed_dim),\n","            nn.BatchNorm1d(self.vit_embed_dim),\n","            nn.GELU()\n","        )\n","\n","        # 3. FFT feature extractor\n","        self.fft_extractor = FrequencyFeatureExtractor(output_dim=self.vit_embed_dim)\n","\n","        total_fused_dim = 3 * self.vit_embed_dim\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(total_fused_dim, 1024),\n","            nn.BatchNorm1d(1024),  # BatchNorm\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(1024, 512),\n","            nn.BatchNorm1d(512),   # BatchNorm\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(512, num_classes)\n","        )\n","\n","        self._set_trainable_layers()\n","\n","    def _set_trainable_layers(self):\n","        # --- ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë‘ ê³ ì • ---\n","        for param in self.vit_model.parameters():\n","            param.requires_grad = False\n","        for param in self.cnn_features.parameters():\n","            param.requires_grad = False\n","        for name, module in self.fft_extractor.named_children():\n","             # spectrum_processor ë“± ë‚´ë¶€ ë¡œì§ ê³ ì •\n","            for param in module.parameters():\n","                param.requires_grad = False\n","\n","        # --- ì¼ë¶€ ë§ˆì§€ë§‰ ë¸”ë¡ë§Œ ê³ ì • í•´ì œ (Unfreeze) ---\n","\n","        # 1. ViTì˜ ë§ˆì§€ë§‰ 1ê°œ ë¸”ë¡ë§Œ í•™ìŠµ\n","        layers_to_train = 1\n","        print(f\"INFO: ViTì˜ ë§ˆì§€ë§‰ {layers_to_train}ê°œ ë¸”ë¡ì˜ ê³ ì •ì„ í•´ì œí•©ë‹ˆë‹¤.\")\n","        for block in self.vit_model.blocks[-layers_to_train:]:\n","            for param in block.parameters():\n","                param.requires_grad = True\n","\n","        # ViTì˜ Layer Norm(ë§ˆì§€ë§‰ ì •ê·œí™”)ë„ í•™ìŠµ\n","        for param in self.vit_model.norm.parameters():\n","            param.requires_grad = True\n","\n","        # 2. DenseNetì˜ ë§ˆì§€ë§‰ DenseBlockë§Œ í•™ìŠµ\n","        # DenseNet121ì€ denseblock1 ~ denseblock4 êµ¬ì¡°ìž…ë‹ˆë‹¤.\n","        print(\"INFO: DenseNetì˜ ë§ˆì§€ë§‰ DenseBlock(4)ì˜ ê³ ì •ì„ í•´ì œí•©ë‹ˆë‹¤.\")\n","        for param in self.cnn_features.denseblock4.parameters():\n","            param.requires_grad = True\n","        for param in self.cnn_features.norm5.parameters(): # ë§ˆì§€ë§‰ Norm\n","            param.requires_grad = True\n","\n","        # --- projection, classifier í•™ìŠµ ---\n","        for param in self.cnn_projection.parameters():\n","            param.requires_grad = True\n","\n","        # FFT Extractor ë‚´ë¶€ì˜ projection ë ˆì´ì–´ê°€ ìžˆë‹¤ë©´ í•™ìŠµ\n","        if hasattr(self.fft_extractor, 'projection'):\n","            for param in self.fft_extractor.projection.parameters():\n","                param.requires_grad = True\n","\n","        for param in self.classifier.parameters():\n","            param.requires_grad = True\n","\n","    def forward(self, x):\n","        # 1. ViT (CLS Token)\n","        vit_features = self.vit_model.forward_features(x)\n","        cls_token = vit_features[:, 0, :] # (B, 768)\n","\n","        # 2. DenseNet\n","        cnn_features_map = self.cnn_features(x)\n","        cnn_features_vec = torch.flatten(self.global_avg_pool(cnn_features_map), 1)\n","        cnn_frequency_features = self.cnn_projection(cnn_features_vec) # (B, 768)\n","\n","        # 3. FFT\n","        fft_frequency_features = self.fft_extractor(x) # (B, 768)\n","\n","        # 4. Fusion\n","        fused_features = torch.cat((cls_token, cnn_frequency_features, fft_frequency_features), dim=1)\n","\n","        # 5. Classifier\n","        logits = self.classifier(fused_features)\n","\n","        return logits"],"metadata":{"id":"73UCD9YXOZgs","executionInfo":{"status":"ok","timestamp":1765688453220,"user_tz":-540,"elapsed":7,"user":{"displayName":"MyeongJin Lee","userId":"03843740464656886981"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import torchvision.transforms.functional as TF\n","\n","class SetFixedContrast(object):\n","    \"\"\"\n","    ì´ë¯¸ì§€ì˜ ëŒ€ë¹„(Contrast)ë¥¼ íŠ¹ì • ìƒìˆ˜ ê°’ìœ¼ë¡œ ê³ ì •í•˜ì—¬ ì„¤ì •í•©ë‹ˆë‹¤.\n","    \"\"\"\n","    def __init__(self, factor=0.1):\n","        self.factor = factor\n","\n","    def __call__(self, img):\n","        # PIL Imageë¥¼ ìž…ë ¥ìœ¼ë¡œ ë°›ì•„ Contrastë¥¼ factorë¡œ ì¡°ì •\n","        return TF.adjust_contrast(img, self.factor)"],"metadata":{"id":"sAhhkZ5Ie_7O","executionInfo":{"status":"ok","timestamp":1765692684840,"user_tz":-540,"elapsed":4,"user":{"displayName":"MyeongJin Lee","userId":"03843740464656886981"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import timm\n","import os\n","from PIL import Image\n","import numpy as np\n","# OpenCVê°€ í•„ìš”í•˜ë¯€ë¡œ import í•©ë‹ˆë‹¤.\n","import cv2\n","\n","# ==========================================\n","# 1. ì„¤ì • (Configuration)\n","# ==========================================\n","# splitfoldersë¡œ ìƒì„±ëœ TEST ê²½ë¡œ\n","TEST_DIR = '/kaggle/working/Splitted/test'\n","\n","# í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (Fine-tuning ê²°ê³¼ íŒŒì¼)\n","MODEL_PATH = '/content/drive/MyDrive/ComputerVision/best_material_classifier_epoch_6.pth'\n","\n","BATCH_SIZE = 32\n","NUM_CLASSES = 23\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# ==========================================\n","# 2. ì‹¤í—˜ìš© ì»¤ìŠ¤í…€ Transform ì •ì˜\n","# ==========================================\n","\n","class FrequencyFeatureExtractor(nn.Module):\n","    \"\"\"\n","    2D FFTë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì˜ í¬ê¸°(Magnitude) ìŠ¤íŽ™íŠ¸ëŸ¼ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³  ì²˜ë¦¬í•˜ëŠ” ëª¨ë“ˆ\n","    \"\"\"\n","    def __init__(self, output_dim, image_size=224):\n","        super(FrequencyFeatureExtractor, self).__init__()\n","        self.image_size = image_size\n","\n","        self.spectrum_processor = nn.Sequential(\n","            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),\n","            nn.AdaptiveAvgPool2d((1, 1))\n","        )\n","\n","        # ìµœì¢…ì ìœ¼ë¡œ ViT ìž„ë² ë”© ì°¨ì›(output_dim)ì— ë§žì¶”ê¸° ìœ„í•œ íˆ¬ì˜ ë ˆì´ì–´\n","        self.projection = nn.Linear(64, output_dim)\n","\n","    def forward(self, x):\n","        # 1. RGB ì´ë¯¸ì§€ë¥¼ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜\n","        x_gray = torch.mean(x, dim=1, keepdim=True)\n","\n","        # 2. 2D FFT ì ìš© ë° ì¤‘ì‹¬ ì´ë™\n","        f = torch.fft.fft2(x_gray, dim=(-2, -1))\n","        f_shifted = torch.fft.fftshift(f, dim=(-2, -1))\n","\n","        # 3. í¬ê¸° ìŠ¤íŽ™íŠ¸ëŸ¼ (Magnitude Spectrum) ê³„ì‚°\n","        magnitude_spectrum = torch.log(1 + torch.abs(f_shifted))\n","\n","        # 4. ìŠ¤íŽ™íŠ¸ëŸ¼ ì²˜ë¦¬ ë° ë²¡í„°í™”\n","        processed_features = self.spectrum_processor(magnitude_spectrum)\n","        processed_features = torch.flatten(processed_features, 1)\n","\n","        # 5. ViT ìž„ë² ë”© ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜\n","        frequency_features = self.projection(processed_features)\n","\n","        return frequency_features\n","\n","# 2-2. Fine-Tuned Fusion Classifier (í•™ìŠµ ì½”ë“œì—ì„œ ê°€ì ¸ì˜´)\n","class FineTunedFusionClassifier(nn.Module):\n","    def __init__(self, num_classes=23, vit_model_name='vit_base_patch16_224', vit_weight_path=None):\n","        super(FineTunedFusionClassifier, self).__init__()\n","\n","        # 1. ViT ëª¨ë¸ (timm)\n","        self.vit_model = timm.create_model(\n","            vit_model_name,\n","            pretrained=False, # í•™ìŠµ ì‹œì™€ ë™ì¼í•˜ê²Œ\n","            num_classes=0,\n","        )\n","        self.vit_embed_dim = self.vit_model.num_features # 768\n","\n","        # ViT ê°€ì¤‘ì¹˜ ë¡œë“œ ë¶€ë¶„ì€ í…ŒìŠ¤íŠ¸ì—ì„œëŠ” ê±´ë„ˆëœë‹ˆë‹¤.\n","\n","        # 2. DenseNet íŠ¹ì§• ì¶”ì¶œê¸°\n","        self.cnn_model = densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)\n","        self.cnn_features = self.cnn_model.features\n","        self.cnn_feature_dim = self.cnn_model.classifier.in_features # 1024\n","\n","        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n","\n","        # projection\n","        self.cnn_projection = nn.Sequential(\n","            nn.Linear(self.cnn_feature_dim, self.vit_embed_dim),\n","            nn.BatchNorm1d(self.vit_embed_dim),\n","            nn.GELU()\n","        )\n","\n","        # 3. FFT feature extractor (ìœ„ì— ì •ì˜í•œ í´ëž˜ìŠ¤ ì‚¬ìš©)\n","        self.fft_extractor = FrequencyFeatureExtractor(output_dim=self.vit_embed_dim)\n","\n","        total_fused_dim = 3 * self.vit_embed_dim\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(total_fused_dim, 1024),\n","            nn.BatchNorm1d(1024),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(1024, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(512, num_classes)\n","        )\n","\n","        # _set_trainable_layers í•¨ìˆ˜ëŠ” ê°€ì¤‘ì¹˜ ë¡œë“œì— ì˜í–¥ì„ ì£¼ì§€ ì•Šìœ¼ë¯€ë¡œ ì •ì˜ë§Œ ìœ ì§€\n","        self._set_trainable_layers()\n","\n","    def _set_trainable_layers(self):\n","        # í•™ìŠµ ì½”ë“œì˜ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤.\n","        for param in self.vit_model.parameters():\n","            param.requires_grad = False\n","        for param in self.cnn_features.parameters():\n","            param.requires_grad = False\n","        for name, module in self.fft_extractor.named_children():\n","            for param in module.parameters():\n","                param.requires_grad = False\n","\n","        layers_to_train = 1\n","        for block in self.vit_model.blocks[-layers_to_train:]:\n","            for param in block.parameters():\n","                param.requires_grad = True\n","\n","        for param in self.vit_model.norm.parameters():\n","            param.requires_grad = True\n","\n","        for param in self.cnn_features.denseblock4.parameters():\n","            param.requires_grad = True\n","        for param in self.cnn_features.norm5.parameters():\n","            param.requires_grad = True\n","\n","        for param in self.cnn_projection.parameters():\n","            param.requires_grad = True\n","\n","        if hasattr(self.fft_extractor, 'projection'):\n","            for param in self.fft_extractor.projection.parameters():\n","                param.requires_grad = True\n","\n","        for param in self.classifier.parameters():\n","            param.requires_grad = True\n","\n","    def forward(self, x):\n","        # 1. ViT (CLS Token)\n","        vit_features = self.vit_model.forward_features(x)\n","        cls_token = vit_features[:, 0, :]\n","\n","        # 2. DenseNet\n","        cnn_features_map = self.cnn_features(x)\n","        cnn_features_vec = torch.flatten(self.global_avg_pool(cnn_features_map), 1)\n","        cnn_frequency_features = self.cnn_projection(cnn_features_vec)\n","\n","        # 3. FFT\n","        fft_frequency_features = self.fft_extractor(x)\n","\n","        # 4. Fusion\n","        fused_features = torch.cat((cls_token, cnn_frequency_features, fft_frequency_features), dim=1)\n","\n","        # 5. Classifier\n","        logits = self.classifier(fused_features)\n","\n","        return logits\n","\n","\n","# ==========================================\n","# 3. ì‹¤í—˜ìš© ì»¤ìŠ¤í…€ Transform ì •ì˜ (ë³€ê²½ ì—†ìŒ)\n","# ==========================================\n","\n","class AddGaussianNoise(object):\n","    \"\"\"ì´ë¯¸ì§€ í…ì„œì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ëŠ” í´ëž˜ìŠ¤\"\"\"\n","    def __init__(self, mean=0., std=0.1):\n","        self.std = std\n","        self.mean = mean\n","\n","    def __call__(self, tensor):\n","        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n","\n","# Shape ì œê±°: 64x64 íŒ¨ì¹˜ ëžœë¤ ì…”í”Œ\n","class RemoveShapeByPatchShuffle(object):\n","    \"\"\"\n","    ì´ë¯¸ì§€ë¥¼ 64x64 íŒ¨ì¹˜ë¡œ ë‚˜ëˆ„ê³  ëžœë¤í•˜ê²Œ ì„žì–´ í˜•íƒœ ì •ë³´ë¥¼ ì œê±°í•©ë‹ˆë‹¤.\n","    ìž…ë ¥: PIL Image (256x256 í¬ê¸°ê°€ ì˜ˆìƒë¨)\n","    ì¶œë ¥: PIL Image\n","    \"\"\"\n","    def __init__(self, patch_size=64):\n","        self.patch_size = patch_size\n","\n","    def __call__(self, img):\n","        # PIL Imageë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜\n","        img_np = np.array(img)\n","        H, W, C = img_np.shape\n","        P = self.patch_size\n","\n","        # 256x256ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆí–ˆë‹¤ë©´ Hì™€ WëŠ” 256ì´ì–´ì•¼ í•¨\n","        if H % P != 0 or W % P != 0:\n","            # ë¦¬ì‚¬ì´ì¦ˆê°€ ì œëŒ€ë¡œ ì•ˆ ëë‹¤ë©´, ë¡œì§ ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ê²½ê³  ëŒ€ì‹  ì—ëŸ¬ ë°œìƒ ë˜ëŠ” 64ì˜ ë°°ìˆ˜ë¡œ ê°•ì œ ì¡°ì •\n","            print(f\"CRITICAL WARNING: Image size {H}x{W} is NOT divisible by patch size {P}. Skipping shuffle.\")\n","            return img # ì…”í”Œ ì—†ì´ ì›ë³¸ ë°˜í™˜ (ì‹¤í—˜ ì¡°ê±´ ìœ„ë°˜)\n","\n","        # ì´ë¯¸ì§€ íŒ¨ì¹˜ë¡œ ë‚˜ëˆ„ê¸°, ì…”í”Œ, ìž¬êµ¬ì„± ë¡œì§ì€ ë™ì¼\n","        # ... (ì´í•˜ íŒ¨ì¹˜ ì…”í”Œ ë¡œì§)\n","        patches = img_np.reshape(H // P, P, W // P, P, C)\n","        patches = patches.swapaxes(1, 2)\n","        patches = patches.reshape(-1, P, P, C)\n","\n","        indices = np.arange(len(patches))\n","        np.random.shuffle(indices)\n","        shuffled_patches = patches[indices]\n","\n","        num_H_patches = H // P\n","        num_W_patches = W // P\n","\n","        shuffled_patches = shuffled_patches.reshape(num_H_patches, num_W_patches, P, P, C)\n","        shuffled_patches = shuffled_patches.swapaxes(1, 2)\n","        reconstructed_img_np = shuffled_patches.reshape(H, W, C)\n","\n","        reconstructed_img_np = reconstructed_img_np.astype(np.uint8)\n","\n","        # NumPy ë°°ì—´ì„ ë‹¤ì‹œ PIL Imageë¡œ ë³€í™˜\n","        return Image.fromarray(reconstructed_img_np)\n","\n","\n","\n","# Texture ì œê±° í›„ Edge ë³µì› (OpenCV ì‚¬ìš©)\n","class TextureRemovalEdgeRestoration(object):\n","    def __call__(self, img):\n","        img_bgr = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n","\n","        # â‘  Texture ì œê±°: Gaussian Blur (9x9 ì»¤ë„)\n","        blur = cv2.GaussianBlur(img_bgr, (9, 9), 0)\n","\n","        # â‘¡ Edge ë³µì›: Canny Edge Detection\n","        blur_gray = cv2.cvtColor(blur, cv2.COLOR_BGR2GRAY)\n","        edges = cv2.Canny(blur_gray, 100, 200)\n","\n","        # ë‹¤ì‹œ 3ì±„ë„ BGRë¡œ ë³€í™˜\n","        edges_bgr = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n","\n","        # â‘¢ ìµœì¢… í•©ì„± ë‹¨ê³„: Weighted Addition\n","        result_bgr = cv2.addWeighted(blur, 0.8, edges_bgr, 0.2, 0)\n","\n","        return Image.fromarray(cv2.cvtColor(result_bgr, cv2.COLOR_BGR2RGB))\n","\n","# Color ì œê±° (Grayscale ë³€í™˜ í›„ 3ì±„ë„ ë³µì œ)\n","class RemoveColorToGrayscale(object):\n","    def __call__(self, img):\n","        img_gray = img.convert('L')\n","        img_np = np.array(img_gray)\n","        img_3channel = np.stack((img_np,)*3, axis=-1)\n","        return Image.fromarray(img_3channel)\n","\n","\n","def get_transforms(mode='clean'):\n","    \"\"\" modeì— ë”°ë¼ ë‹¤ë¥¸ ì „ì²˜ë¦¬ë¥¼ ë°˜í™˜ \"\"\"\n","\n","    final_transforms = [\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ]\n","\n","    if mode == 'clean':\n","        return transforms.Compose(final_transforms)\n","\n","    elif mode == 'illumination':\n","        # ì¡°ëª… ë³€í™”: Contrastë¥¼ 0.1ë¡œ ê³ ì •\n","        return transforms.Compose([\n","            # Contrastë¥¼ 0.1ë¡œ ê³ ì •í•˜ì—¬ ì ìš©\n","            transforms.Resize((224, 224)), # ë¨¼ì € ëª¨ë¸ ìž…ë ¥ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ\n","            SetFixedContrast(factor=0.1), # ëŒ€ë¹„ 0.1ë¡œ ê³ ì • (ë§¤ìš° ë‚®ì€ ëŒ€ë¹„)\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ])\n","\n","    elif mode == 'shape_removed':\n","        # 1. 256x256ìœ¼ë¡œ ë¦¬ì‚¬ì´ì§• (64ë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì§€ê²Œ í•˜ê¸° ìœ„í•¨)\n","        # 2. 64x64 íŒ¨ì¹˜ ì…”í”Œ\n","        # 3. ìµœì¢… í¬ê¸°(224x224)ë¡œ ë¦¬ì‚¬ì´ì§• ë° í…ì„œ ë³€í™˜/ì •ê·œí™”\n","        return transforms.Compose([\n","            transforms.Resize((256, 256)),\n","            RemoveShapeByPatchShuffle(patch_size=64), # 64x64 íŒ¨ì¹˜ ì‚¬ìš©\n","            *final_transforms\n","        ])\n","\n","    elif mode == 'texture_edge':\n","        return transforms.Compose([\n","            TextureRemovalEdgeRestoration(),\n","            *final_transforms\n","        ])\n","\n","    elif mode == 'color_removed':\n","        return transforms.Compose([\n","            RemoveColorToGrayscale(),\n","            *final_transforms\n","        ])\n","\n","\n","\n","    else:\n","        raise ValueError(f\"Unknown mode: {mode}\")\n","\n","# ==========================================\n","# 4. í‰ê°€ í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ)\n","# ==========================================\n","def evaluate(model, data_loader):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in data_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    return 100 * correct / total\n","\n","# ==========================================\n","# 5. ë©”ì¸ ì‹¤í–‰ ì½”ë“œ (Fusion Classifier ë¡œë“œ)\n","# ==========================================\n","if __name__ == '__main__':\n","    print(f\"Using device: {device}\")\n","\n","    # 1. ëª¨ë¸ êµ¬ì¡° ìƒì„±: FineTunedFusionClassifierë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n","    model = FineTunedFusionClassifier(num_classes=NUM_CLASSES)\n","\n","    # 2. í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ\n","    if os.path.exists(MODEL_PATH):\n","        print(f\"Loading Fine-tuned FusionClassifier weights from {MODEL_PATH}...\")\n","\n","        state_dict = torch.load(MODEL_PATH, map_location=device)\n","\n","        # ê°€ì¤‘ì¹˜ íŒŒì¼ì´ 'model' í‚¤ ì•„ëž˜ì— ì €ìž¥ëœ ê²½ìš° ì²˜ë¦¬\n","        if 'model' in state_dict:\n","            state_dict = state_dict['model']\n","\n","        # FusionClassifier êµ¬ì¡°ì— ì „ì²´ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. (ì˜¤ë¥˜ í•´ê²°)\n","        try:\n","            model.load_state_dict(state_dict, strict=True)\n","            print(\"INFO: Model weights loaded successfully.\")\n","        except RuntimeError as e:\n","            print(f\"CRITICAL ERROR: Failed to load state_dict for FusionClassifier.\")\n","            print(f\"Error details: {e}\")\n","            exit()\n","\n","    else:\n","        print(f\"Error: {MODEL_PATH} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! í•™ìŠµ ë¨¼ì € ì™„ë£Œí•´ì£¼ì„¸ìš”.\")\n","        exit()\n","\n","    model.to(device)\n","\n","    # 3. ì‹¤í—˜ ëª¨ë“œë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n","    modes = ['clean', 'shape_removed', 'texture_edge', 'color_removed', 'illumination']\n","    #modes = ['clean','illumination']\n","\n","    results = {}\n","\n","    print(\"\\n\" + \"=\"*50)\n","    print(\" Â  Â  Â  STARTING MATERIAL CLASSIFICATION ROBUSTNESS TEST Â  Â  Â \")\n","    print(\"=\"*50)\n","\n","    for mode in modes:\n","        print(f\"\\nTesting Mode: [{mode.upper()}]\")\n","\n","        transform = get_transforms(mode)\n","\n","        try:\n","            test_dataset = datasets.ImageFolder(root=TEST_DIR, transform=transform)\n","            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","            if len(test_dataset) == 0:\n","                print(\"Error: Test í´ë”ê°€ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n","                continue\n","\n","            acc = evaluate(model, test_loader)\n","            results[mode] = acc\n","            print(f\"--> Accuracy: {acc:.2f}%\")\n","\n","        except FileNotFoundError:\n","            print(f\"Error: {TEST_DIR} ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n","            break\n","\n","    # 4. ìµœì¢… ê²°ê³¼ ë¦¬í¬íŠ¸\n","    print(\"\\n\" + \"=\"*50)\n","    print(\" Â  Â  Â  Â  Â  FINAL RESULTS REPORT Â  Â  Â  Â \")\n","    print(\"=\"*50)\n","\n","    base_acc = results.get('clean', 0.0)\n","    print(f\"Original Accuracy (Clean): {base_acc:.2f}%\")\n","\n","    for mode in modes:\n","        if mode == 'clean': continue\n","        if mode in results:\n","            acc = results[mode]\n","            drop = base_acc - acc\n","            print(f\"- {mode.upper()} Accuracy: {acc:.2f}% Â (Drop: -{drop:.2f}%)\")\n","\n","    print(\"=\"*50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CghmnpnCLndn","executionInfo":{"status":"ok","timestamp":1765702264841,"user_tz":-540,"elapsed":452097,"user":{"displayName":"MyeongJin Lee","userId":"03843740464656886981"}},"outputId":"8fc8b604-00ef-4234-f790-e04416c2807b"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Loading Fine-tuned FusionClassifier weights from /content/drive/MyDrive/ComputerVision/best_material_classifier_epoch_6.pth...\n","INFO: Model weights loaded successfully.\n","\n","==================================================\n"," Â  Â  Â  STARTING MATERIAL CLASSIFICATION ROBUSTNESS TEST Â  Â  Â \n","==================================================\n","\n","Testing Mode: [CLEAN]\n","--> Accuracy: 88.56%\n","\n","Testing Mode: [SHAPE_REMOVED]\n","--> Accuracy: 46.96%\n","\n","Testing Mode: [TEXTURE_EDGE]\n","--> Accuracy: 84.33%\n","\n","Testing Mode: [COLOR_REMOVED]\n","--> Accuracy: 82.28%\n","\n","Testing Mode: [ILLUMINATION]\n","--> Accuracy: 62.03%\n","\n","==================================================\n"," Â  Â  Â  Â  Â  FINAL RESULTS REPORT Â  Â  Â  Â \n","==================================================\n","Original Accuracy (Clean): 88.56%\n","- SHAPE_REMOVED Accuracy: 46.96% Â (Drop: -41.60%)\n","- TEXTURE_EDGE Accuracy: 84.33% Â (Drop: -4.23%)\n","- COLOR_REMOVED Accuracy: 82.28% Â (Drop: -6.28%)\n","- ILLUMINATION Accuracy: 62.03% Â (Drop: -26.52%)\n","==================================================\n"]}]},{"cell_type":"markdown","source":["# ì•„ëž˜ëŠ” ìƒ˜í”Œ ì´ë¯¸ì§€ ë½‘ê¸°ìš©"],"metadata":{"id":"GVe4IZmNxCbr"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import timm\n","import os\n","from PIL import Image\n","import numpy as np\n","import cv2\n","import torchvision.transforms.functional as TF # Contrast ê³ ì •ì— í•„ìš”\n","\n","# ==========================================\n","# 1. ì„¤ì • (Configuration)\n","# ==========================================\n","# splitfoldersë¡œ ìƒì„±ëœ TEST ê²½ë¡œ\n","TEST_DIR = '/kaggle/working/Splitted/test'\n","\n","SPECIFIC_SAMPLE_PATH = '/content/drive/MyDrive/ComputerVision/brick_000006.jpg'\n","\n","# í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (Fine-tuning ê²°ê³¼ íŒŒì¼)\n","MODEL_PATH = '/content/drive/MyDrive/ComputerVision/best_material_classifier_epoch_6.pth'\n","\n","BATCH_SIZE = 32\n","NUM_CLASSES = 23\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# ì´ë¯¸ì§€ ì €ìž¥ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ ìƒì„±\n","OUTPUT_DIR = './transformed_samples'\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","\n","# ==========================================\n","# 2. ëª¨ë¸ ë° ì»¤ìŠ¤í…€ Transform ì •ì˜ (ì¼ë¶€ ìƒëžµ ë° ì¶”ê°€)\n","# ==========================================\n","\n","# ... (FrequencyFeatureExtractor ë° FineTunedFusionClassifier í´ëž˜ìŠ¤ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€) ...\n","from torchvision.models import DenseNet121_Weights, densenet121\n","# ì´ ë¶€ë¶„ì€ ì‚¬ìš©ìžë‹˜ì˜ ì½”ë“œ ìƒë‹¨ì— í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n","\n","# ==========================================\n","# 2-A. ILLUMINATION ëª¨ë“œìš© ì»¤ìŠ¤í…€ Transform ì¶”ê°€\n","# ==========================================\n","class SetFixedContrast(object):\n","    \"\"\"\n","    ì´ë¯¸ì§€ì˜ ëŒ€ë¹„(Contrast)ë¥¼ íŠ¹ì • ìƒìˆ˜ ê°’ìœ¼ë¡œ ê³ ì •í•˜ì—¬ ì„¤ì •í•©ë‹ˆë‹¤.\n","    \"\"\"\n","    def __init__(self, factor=0.1):\n","        self.factor = factor\n","\n","    def __call__(self, img):\n","        # PIL Imageë¥¼ ìž…ë ¥ìœ¼ë¡œ ë°›ì•„ Contrastë¥¼ factorë¡œ ì¡°ì •\n","        return TF.adjust_contrast(img, self.factor)\n","\n","\n","# ==========================================\n","# 3. ì‹¤í—˜ìš© ì»¤ìŠ¤í…€ Transform ì •ì˜ (get_transforms í•¨ìˆ˜ë§Œ ìˆ˜ì •)\n","# ==========================================\n","\n","# ... (AddGaussianNoise, RemoveShapeByPatchShuffle ë“± í´ëž˜ìŠ¤ ì •ì˜ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€) ...\n","\n","\n","def get_transforms(mode='clean'):\n","  \"\"\" modeì— ë”°ë¼ ë‹¤ë¥¸ ì „ì²˜ë¦¬ë¥¼ ë°˜í™˜ \"\"\"\n","\n","  final_transforms = [transforms.Resize((224, 224)),transforms.ToTensor(),\n","                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n","  if mode == 'clean':\n","    return transforms.Compose(final_transforms)\n","\n","  elif mode == 'illumination':\n","    # ì¡°ëª… ë³€í™”: Contrastë¥¼ 0.1ë¡œ ê³ ì •\n","    return transforms.Compose([\n","        # Contrast 0.1 ê³ ì • ì‹œ PIL Imageë¥¼ ì²˜ë¦¬í•´ì•¼ í•˜ë¯€ë¡œ Resizeë¥¼ ë¨¼ì € ì ìš©í•©ë‹ˆë‹¤.\n","        transforms.Resize((224, 224)),\n","        SetFixedContrast(factor=0.1), # ëŒ€ë¹„ 0.1ë¡œ ê³ ì • (ë§¤ìš° ë‚®ì€ ëŒ€ë¹„)\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n","\n","  elif mode == 'shape_removed':\n","    # 1. 256x256ìœ¼ë¡œ ë¦¬ì‚¬ì´ì§• (64ë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì§€ê²Œ í•˜ê¸° ìœ„í•¨)\n","    # 2. 64x64 íŒ¨ì¹˜ ì…”í”Œ\n","    # 3. ìµœì¢… í¬ê¸°(224x224)ë¡œ ë¦¬ì‚¬ì´ì§• ë° í…ì„œ ë³€í™˜/ì •ê·œí™”\n","    return transforms.Compose([\n","        transforms.Resize((256, 256)),\n","        RemoveShapeByPatchShuffle(patch_size=64), # 64x64 íŒ¨ì¹˜ ì‚¬ìš©\n","        *final_transforms])\n","\n","  elif mode == 'texture_edge':\n","    return transforms.Compose([\n","        TextureRemovalEdgeRestoration(),\n","        *final_transforms])\n","\n","  elif mode == 'color_removed':\n","    return transforms.Compose([\n","        RemoveColorToGrayscale(),\n","        *final_transforms\n","        ])\n","\n","  else:\n","    raise ValueError(f\"Unknown mode: {mode}\")\n","\n","\n","# ==========================================\n","# 4. ì´ë¯¸ì§€ ìƒ˜í”Œ ì €ìž¥ í•¨ìˆ˜ (ìƒˆë¡œ ì¶”ê°€)\n","# ==========================================\n","\n","def save_sample_image(mode, transform, specific_file_path, output_dir='./transformed_samples'):\n","    \"\"\"\n","    ì§€ì •ëœ Google Drive íŒŒì¼ ê²½ë¡œì˜ ì´ë¯¸ì§€ë¥¼ ì°¾ì•„ ë³€í™˜ì„ ì ìš©í•˜ê³  ì €ìž¥í•©ë‹ˆë‹¤.\n","    \"\"\"\n","\n","    # 1. Google Drive ê²½ë¡œì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ\n","    try:\n","        # ê²½ë¡œì—ì„œ íŒŒì¼ ì´ë¦„ê³¼ ê¸°ë³¸ ì´ë¦„ ì¶”ì¶œ\n","        file_name = os.path.basename(specific_file_path)\n","        base_name = os.path.splitext(file_name)[0]\n","\n","        # PIL Imageë¥¼ ë¡œë“œ\n","        sample_img_pil = Image.open(specific_file_path).convert('RGB')\n","\n","    except Exception as e:\n","        print(f\"CRITICAL ERROR: Failed to load sample image from {specific_file_path}.\")\n","        print(f\"Error details: {e}\")\n","        return\n","\n","    # 2. ë³€í™˜ ì ìš© (ToTensor()ì™€ Normalize()ë¥¼ ì œì™¸í•œ PIL ë‹¨ê³„ì˜ Transform)\n","    try:\n","        # NOTE: ì´ ë¶€ë¶„ì€ get_transforms í•¨ìˆ˜ì™€ ë¡œì§ì´ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.\n","        if mode == 'clean':\n","            display_transforms = transforms.Resize((224, 224))\n","\n","        elif mode == 'illumination':\n","            display_transforms = transforms.Compose([\n","                transforms.Resize((224, 224)),\n","                SetFixedContrast(factor=0.1),\n","            ])\n","\n","        elif mode == 'shape_removed':\n","            display_transforms = transforms.Compose([\n","                transforms.Resize((256, 256)),\n","                RemoveShapeByPatchShuffle(patch_size=4),\n","                transforms.Resize((224, 224)),\n","            ])\n","\n","        elif mode == 'texture_edge':\n","            display_transforms = transforms.Compose([\n","                TextureRemovalEdgeRestoration(),\n","                transforms.Resize((224, 224)),\n","            ])\n","\n","        elif mode == 'color_removed':\n","            display_transforms = transforms.Compose([\n","                RemoveColorToGrayscale(),\n","                transforms.Resize((224, 224)),\n","            ])\n","\n","        else:\n","            print(f\"WARNING: Unknown mode {mode} for saving sample.\")\n","            return\n","\n","        transformed_img_pil = display_transforms(sample_img_pil)\n","\n","        # 3. ì´ë¯¸ì§€ ì €ìž¥\n","        os.makedirs(output_dir, exist_ok=True)\n","        save_path = os.path.join(output_dir, f\"{base_name}_{mode.upper()}.png\")\n","        transformed_img_pil.save(save_path)\n","\n","        print(f\"--> Saved sample image for [{mode.upper()}] to: {save_path}\")\n","\n","    except Exception as e:\n","        print(f\"Error applying transform or saving sample for {mode.upper()}: {e}\")\n","\n","    return\n","\n","# ==========================================\n","# 5. ë©”ì¸ ì‹¤í–‰ ì½”ë“œ (Fusion Classifier ë¡œë“œ ë° ìƒ˜í”Œ ì €ìž¥)\n","# ==========================================\n","if __name__ == '__main__':\n","    # ... (ëª¨ë¸ ë¡œë“œ ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€) ...\n","    # 3. ì‹¤í—˜ ëª¨ë“œë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n","    #modes = ['clean', 'shape_removed', 'texture_edge', 'color_removed', 'illumination']\n","    modes = ['shape_removed']\n","    results = {}\n","\n","    print(\"\\n\" + \"=\"*50)\n","    print(\" Â  Â  Â  STARTING MATERIAL CLASSIFICATION ROBUSTNESS TEST Â  Â  Â \")\n","    print(\"=\"*50)\n","    print(f\"Sample images will be saved in: {OUTPUT_DIR}\")\n","\n","    for mode in modes:\n","        print(f\"\\nTesting Mode: [{mode.upper()}]\")\n","\n","        transform = get_transforms(mode)\n","\n","        # ðŸŒŸ ëª¨ë“œë³„ ë³€í™˜ëœ ìƒ˜í”Œ ì´ë¯¸ì§€ ì €ìž¥ ðŸŒŸ\n","        #save_sample_image(mode, transform)\n","        save_sample_image(mode, transform, SPECIFIC_SAMPLE_PATH, OUTPUT_DIR)\n","\n","        try:\n","            test_dataset = datasets.ImageFolder(root=TEST_DIR, transform=transform)\n","            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","            if len(test_dataset) == 0:\n","                print(\"Error: Test í´ë”ê°€ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n","                continue\n","\n","            acc = evaluate(model, test_loader)\n","            results[mode] = acc\n","            print(f\"--> Accuracy: {acc:.2f}%\")\n","\n","        except FileNotFoundError:\n","            print(f\"Error: {TEST_DIR} ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n","            break\n","\n","    # 4. ìµœì¢… ê²°ê³¼ ë¦¬í¬íŠ¸... (ì´í•˜ ë™ì¼)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"do8pMYulxEwu","executionInfo":{"status":"ok","timestamp":1765701728834,"user_tz":-540,"elapsed":90228,"user":{"displayName":"MyeongJin Lee","userId":"03843740464656886981"}},"outputId":"a0ec1866-7427-45f1-a891-3a860d4fcb72"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","==================================================\n"," Â  Â  Â  STARTING MATERIAL CLASSIFICATION ROBUSTNESS TEST Â  Â  Â \n","==================================================\n","Sample images will be saved in: ./transformed_samples\n","\n","Testing Mode: [SHAPE_REMOVED]\n","--> Saved sample image for [SHAPE_REMOVED] to: ./transformed_samples/brick_000006_SHAPE_REMOVED.png\n","--> Accuracy: 46.05%\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyMqpzu25HTcGsXAc+vq5z2Z"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
