{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1A5ktcoCxQwdYP0H6jIcUcrPwK5ao0Zzo",
      "authorship_tag": "ABX9TyMf6zwDs2+G2ZDVa89PrP/d"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNj4CohRR3LV",
        "outputId": "9191dfaf-3a78-48f0-c9c5-791417beab75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install split-folders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_ULcEZYNcZJ",
        "outputId": "857b49ff-60d0-4e0b-ea6f-576f4bc862e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import DenseNet121_Weights, densenet121\n",
        "from transformers import ViTModel, ViTConfig\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "import kagglehub\n",
        "import shutil\n",
        "import os\n",
        "import timm"
      ],
      "metadata": {
        "id": "l2eYul24S_ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "liewyousheng_minc2500_path = kagglehub.dataset_download('liewyousheng/minc2500')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5x16L6ZM1la",
        "outputId": "b412bbb9-de14-44e3-c2dd-c5a3e9cecc98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/liewyousheng/minc2500?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.10G/2.10G [00:12<00:00, 184MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = liewyousheng_minc2500_path"
      ],
      "metadata": {
        "id": "sc3IMxfJNPKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_dir = os.path.join(base_path, \"minc-2500\", \"images\")\n",
        "\n",
        "selected_classes = ['brick', 'carpet', 'ceramic', 'fabric', 'foliage', 'food', 'glass', 'hair', 'leather',\n",
        "                    'metal', 'mirror', 'other', 'painted', 'paper', 'plastic', 'polishedstone', 'skin',\n",
        "                    'sky', 'stone', 'tile', 'wallpaper', 'water', 'wood']\n",
        "\n",
        "new_dir = \"/kaggle/working/selected_images\"\n",
        "\n",
        "os.makedirs(new_dir, exist_ok=True)\n",
        "\n",
        "for class_name in selected_classes:\n",
        "    class_dir = os.path.join(source_dir, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        shutil.copytree(class_dir, os.path.join(new_dir, class_name))"
      ],
      "metadata": {
        "id": "XFzINdQhNQX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import splitfolders\n",
        "splitfolders.ratio(new_dir, output=\"/kaggle/working/Splitted\", seed=1337, ratio=(0.85, 0.05, 0.10))\n",
        "\n",
        "print(\"Selected classes have been split into train, validation, and test sets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0ffqlvuNiOc",
        "outputId": "8e5ce425-1fc7-458a-fb98-412bef4b6c98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 57500 files [00:07, 7471.20 files/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected classes have been split into train, validation, and test sets.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# path 바꿀 필요 없음\n",
        "\n",
        "train_path = '/kaggle/working/Splitted/train'\n",
        "val_path = '/kaggle/working/Splitted/val'\n",
        "test_path = '/kaggle/working/Splitted/test'"
      ],
      "metadata": {
        "id": "U6EPbku4Nvl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 224\n",
        "TRAIN_BATCH_SIZE = 512\n",
        "VAL_BATCH_SIZE = 512\n",
        "TEST_BATCH_SIZE = 512"
      ],
      "metadata": {
        "id": "zbKpWLQ7RfEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]"
      ],
      "metadata": {
        "id": "LmJcmqlVR-eK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomAffine(degrees=0, scale=(0.8, 1.2)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "])"
      ],
      "metadata": {
        "id": "O0aQr_ZwSATd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    root=train_path,\n",
        "    transform=train_transforms\n",
        ")\n",
        "\n",
        "val_dataset = datasets.ImageFolder(\n",
        "    root=val_path,\n",
        "    transform=val_test_transforms\n",
        ")\n",
        "\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    root=test_path,\n",
        "    transform=val_test_transforms\n",
        ")"
      ],
      "metadata": {
        "id": "NUR3hLviSCUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=VAL_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=TEST_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzO4gkrnSGep",
        "outputId": "56d27a9a-6d3e-4924-c45c-f8e090f040b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터 로더 준비 완료. 배치 크기: 512\n",
            "검증 데이터 로더 준비 완료. 배치 크기: 512\n",
            "테스트 데이터 로더 준비 완료. 배치 크기: 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FrequencyFeatureExtractor(nn.Module):\n",
        "    def __init__(self, output_dim, image_size=224):\n",
        "        super(FrequencyFeatureExtractor, self).__init__()\n",
        "        self.image_size = image_size\n",
        "\n",
        "        self.spectrum_processor = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "        self.projection = nn.Linear(64, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_gray = torch.mean(x, dim=1, keepdim=True)\n",
        "        f = torch.fft.fft2(x_gray, dim=(-2, -1))\n",
        "        f_shifted = torch.fft.fftshift(f, dim=(-2, -1))\n",
        "        magnitude_spectrum = torch.log(1 + torch.abs(f_shifted))\n",
        "\n",
        "        processed_features = self.spectrum_processor(magnitude_spectrum)\n",
        "        processed_features = torch.flatten(processed_features, 1)\n",
        "\n",
        "        frequency_features = self.projection(processed_features)\n",
        "\n",
        "        return frequency_features"
      ],
      "metadata": {
        "id": "Z22MVPhVMUJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "from torchvision.models import DenseNet121_Weights, densenet121\n",
        "import os\n",
        "\n",
        "class FineTunedFusionClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=23, vit_model_name='vit_base_patch16_224', vit_weight_path=None):\n",
        "        super(FineTunedFusionClassifier, self).__init__()\n",
        "\n",
        "        # ViT model\n",
        "        self.vit_model = timm.create_model(\n",
        "            vit_model_name,\n",
        "            pretrained=False,\n",
        "            num_classes=0,\n",
        "        )\n",
        "        self.vit_embed_dim = self.vit_model.num_features # 768\n",
        "\n",
        "        # ViT 가중치 로드\n",
        "        if vit_weight_path and os.path.exists(vit_weight_path):\n",
        "            print(f\"INFO: Custom ViT weights loading from {vit_weight_path}...\")\n",
        "            custom_weights = torch.load(vit_weight_path)\n",
        "            self.vit_model.load_state_dict(custom_weights, strict=False)\n",
        "\n",
        "        # DenseNet\n",
        "        self.cnn_model = densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)\n",
        "        self.cnn_features = self.cnn_model.features\n",
        "        self.cnn_feature_dim = self.cnn_model.classifier.in_features # 1024\n",
        "\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.cnn_projection = nn.Sequential(\n",
        "            nn.Linear(self.cnn_feature_dim, self.vit_embed_dim),\n",
        "            nn.BatchNorm1d(self.vit_embed_dim),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        # FFT feature extractor\n",
        "        self.fft_extractor = FrequencyFeatureExtractor(output_dim=self.vit_embed_dim)\n",
        "\n",
        "        total_fused_dim = 3 * self.vit_embed_dim\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(total_fused_dim, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "        self._set_trainable_layers()\n",
        "\n",
        "    def _set_trainable_layers(self):\n",
        "        for param in self.vit_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.cnn_features.parameters():\n",
        "            param.requires_grad = False\n",
        "        for name, module in self.fft_extractor.named_children():\n",
        "            for param in module.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # ViT의 마지막 1개 블록만 학습\n",
        "        layers_to_train = 1\n",
        "        for block in self.vit_model.blocks[-layers_to_train:]:\n",
        "            for param in block.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "        # ViT의 Layer Norm 학습\n",
        "        for param in self.vit_model.norm.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # DenseNet의 마지막 DenseBlock만 학습\n",
        "        for param in self.cnn_features.denseblock4.parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in self.cnn_features.norm5.parameters(): # 마지막 Norm\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # projection, classifier 학습\n",
        "        for param in self.cnn_projection.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # FFT Extractor 내부의 projection 레이어 학습\n",
        "        if hasattr(self.fft_extractor, 'projection'):\n",
        "            for param in self.fft_extractor.projection.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "        for param in self.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. ViT (CLS Token)\n",
        "        vit_features = self.vit_model.forward_features(x)\n",
        "        cls_token = vit_features[:, 0, :] # (B, 768)\n",
        "\n",
        "        # 2. DenseNet\n",
        "        cnn_features_map = self.cnn_features(x)\n",
        "        cnn_features_vec = torch.flatten(self.global_avg_pool(cnn_features_map), 1)\n",
        "        cnn_frequency_features = self.cnn_projection(cnn_features_vec) # (B, 768)\n",
        "\n",
        "        # 3. FFT\n",
        "        fft_frequency_features = self.fft_extractor(x) # (B, 768)\n",
        "\n",
        "        # 4. Fusion\n",
        "        fused_features = torch.cat((cls_token, cnn_frequency_features, fft_frequency_features), dim=1)\n",
        "\n",
        "        # 5. Classifier\n",
        "        logits = self.classifier(fused_features)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "6djKlTW2qSTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 23\n",
        "NUM_EPOCHS = 60\n",
        "LEARNING_RATE = 1e-4"
      ],
      "metadata": {
        "id": "dqTmljNkQXG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxWN3pe1ShC8",
        "outputId": "5d349ccb-1215-4f71-c0f2-8cace99d90ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "사용 장치: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MINC dataset에 pretrained된 ViT의 pt path로 바꾸기\n",
        "\n",
        "VIT_WEIGHT_PATH = \"YOUR_PATH\""
      ],
      "metadata": {
        "id": "cQkX29SKMAcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FineTunedFusionClassifier(\n",
        "    num_classes=NUM_CLASSES,\n",
        "    vit_weight_path=VIT_WEIGHT_PATH\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "xYqYYdWplizK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learnable_params = filter(lambda p: p.requires_grad, model.parameters())"
      ],
      "metadata": {
        "id": "u9o1pZoCSkV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(learnable_params, lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "CHCd94PJSlL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.1,\n",
        "    patience=5,\n",
        ")"
      ],
      "metadata": {
        "id": "nvprPyQpNE-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# learnable_params = []\n",
        "# for name, param in model.named_parameters():\n",
        "#     if param.requires_grad:\n",
        "#         learnable_params.append(param)\n",
        "#         print(f\"✅ 학습 가능: {name} ({param.numel()}개)\")"
      ],
      "metadata": {
        "id": "MkeXg5p4peeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params_count = sum(p.numel() for p in learnable_params)\n",
        "total_params_count = sum(p.numel() for p in model.parameters())\n",
        "print(f\"총 파라미터 수: {total_params_count / 1e6:.2f}M\")\n",
        "print(f\"최종 학습 가능한 파라미터 수: {trainable_params_count / 1e6:.2f}M\")\n",
        "print(f\"최종 학습 가능한 파라미터 비율: {trainable_params_count / total_params_count * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmV5cJ23np2F",
        "outputId": "006d7b43-204e-4990-cc92-0c4904dc6caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 파라미터 수: 97.53M\n",
            "최종 학습 가능한 파라미터 수: 14.01M\n",
            "최종 학습 가능한 파라미터 비율: 14.37%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for inputs, labels in tqdm(loader, desc=\"Training\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct_predictions += torch.sum(preds == labels.data)\n",
        "        total_samples += inputs.size(0)\n",
        "\n",
        "    epoch_loss = total_loss / total_samples\n",
        "    epoch_acc = correct_predictions.double() / total_samples\n",
        "    return epoch_loss, epoch_acc.item()"
      ],
      "metadata": {
        "id": "041-8WyiSnQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(loader, desc=\"Validation\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct_predictions += torch.sum(preds == labels.data)\n",
        "            total_samples += inputs.size(0)\n",
        "\n",
        "    epoch_loss = total_loss / total_samples\n",
        "    epoch_acc = correct_predictions.double() / total_samples\n",
        "    return epoch_loss, epoch_acc.item()"
      ],
      "metadata": {
        "id": "cmBwxxAwSn9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_acc = 0.0\n",
        "print(\"\\n--- 학습 시작 ---\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS + 1):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # train\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "    # validation\n",
        "    val_loss, val_acc = validate_model(model, val_loader, criterion, device)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    epoch_duration = time.time() - start_time\n",
        "\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"\\n[Epoch {epoch+1}/{NUM_EPOCHS}] Time: {epoch_duration:.2f}s LR: {current_lr:.1e}\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        model_save_path = f'best_material_classifier_epoch_{epoch+1}.pth'\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "        print(f\"  >>> 최적 모델 저장: {model_save_path} (Acc: {best_val_acc:.4f})\")\n",
        "\n",
        "print(\"\\n--- 학습 완료 ---\")\n",
        "print(f\"최고 검증 정확도: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "eR159UnfSplV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}